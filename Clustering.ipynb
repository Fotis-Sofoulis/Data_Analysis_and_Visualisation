{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Part B\n",
    "### Preprocessing of the beacons dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(58633, 4)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('beacons_dataset.csv', delimiter=';')\n",
    "print(df.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "1. Correcting the Room labels"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "rooms = ['Bedroom','Bathroom', 'Livingroom' , 'Kitchen', 'Box', 'Office', 'DinerRoom']\n",
    "#print(\"Unique values of rooms BEFORE fuzzywuzzy\\n\", df['room'].value_counts())\n",
    "def fuzz_ratio(data):\n",
    "        return fuzz.token_sort_ratio(data['room'], rooms[i])\n",
    "\n",
    "for i in range (0, len(rooms)):\n",
    "    changes = df[df.apply(fuzz_ratio, axis=1) > 75].room\n",
    "    df.loc[df['room'].isin(changes), 'room'] = rooms[i]\n",
    "\n",
    "#print(\"Unique values of rooms AFTER fuzzywuzzy\\n\", df['room'].value_counts())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "2. Removing wrong part_id of users"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(46782, 4)\n"
     ]
    }
   ],
   "source": [
    "# Find the wrong IDs if they are not numeric or they are not 4 digit numbers and drop them\n",
    "wrong_id = df[df['part_id'].apply(lambda x: not x.isnumeric() or not int(x) > 999 or not int(x) < 10000)]\n",
    "df.drop(wrong_id.index, axis=0, inplace=True)\n",
    "print(df.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "3. Generating Features"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- Fixing the ts_date column and merging it with ts_time"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "df['ts_date'] = pd.to_datetime(df['ts_date'], format = \"%Y%m%d\").dt.strftime('%Y-%m-%d')\n",
    "# Generate new column with merged date and time\n",
    "df['fulldate'] = pd.to_datetime(df['ts_date'] + \" \" + df['ts_time'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- Generate new column with the difference between row time"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "df['diff_dt_seconds'] = df.sort_values(['part_id','fulldate']).groupby('part_id')['fulldate'].diff().dt.total_seconds()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- Remove NaN values, difference that is zero and higher than 3600 (1 hour)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "# Filter out difference that is NaN, equal to zero, and higher than 3600\n",
    "df = df[df['diff_dt_seconds'].notna()]\n",
    "df = df[df['diff_dt_seconds'] != 0]\n",
    "df = df[df['diff_dt_seconds'] < 3600]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- Calculate percentage of each user time in a room"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "total_sec_id = df.groupby(['part_id'])['diff_dt_seconds'].agg('sum')\n",
    "total_sec_room = df.groupby(['part_id', 'room'])['diff_dt_seconds'].agg('sum')\n",
    "percent_time_id = total_sec_room/total_sec_id*100"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- Create new dataset with part_id and percentage for each room"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "# Break the percentage_time_id into separate columns\n",
    "result_column = percent_time_id.unstack().fillna(0)\n",
    "pre_beacons = pd.DataFrame(result_column)\n",
    "# Keep only the [Bedroom, Bathroom, Livingroom and Kitchen] columns\n",
    "pre_beacons = pre_beacons.loc[:, ['Bedroom', 'Bathroom', 'Livingroom', 'Kitchen']]\n",
    "pre_beacons = pre_beacons.round(1) # round to 1 decimal\n",
    "pre_beacons = pre_beacons.reset_index()\n",
    "\n",
    "pre_beacons.to_csv('preprocessed_beacons.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "2. Merge the preprocessed clinical and beacons datasets by id"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "clinical_df = pd.read_csv('preprocessed_clinical.csv')\n",
    "beacons_df = pd.read_csv('preprocessed_beacons.csv')\n",
    "\n",
    "clinical_df['part_id'] = clinical_df['part_id'].astype(int)\n",
    "beacons_df['part_id'] = beacons_df['part_id'].astype(int)\n",
    "merged_df = pd.merge(clinical_df, beacons_df, on='part_id', how = 'inner')\n",
    "\n",
    "merged_df.to_csv('merged.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "3. Clustering"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, homogeneity_score, v_measure_score, completeness_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "3.1 Initialize K-means, standardize and fit data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=3)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "merged_df_std = scaler.fit_transform(merged_df) # Standardize features\n",
    "\n",
    "kmeans.fit(merged_df_std)\n",
    "kmeans_labels = kmeans.fit_predict(merged_df_std)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "3.2 Evaluation of K-means through metrics"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- Silhouette Score"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette Score : 0.043\n"
     ]
    }
   ],
   "source": [
    "silhouette = silhouette_score(merged_df_std, kmeans_labels)\n",
    "print(\"Silhouette Score :\", silhouette.round(3))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- Homogeneity Score"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Homogeneity  Score : 0.237\n"
     ]
    }
   ],
   "source": [
    "homogeneity = homogeneity_score(merged_df['fried'], kmeans_labels)\n",
    "print (\"Homogeneity  Score :\",homogeneity.round(3))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- V_Measure Score"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V-measure Score : 0.223\n"
     ]
    }
   ],
   "source": [
    "v_measure = v_measure_score(merged_df['fried'], kmeans_labels)\n",
    "print (\"V-measure Score :\",v_measure.round(3))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- Completeness Score"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completeness Score : 0.211\n"
     ]
    }
   ],
   "source": [
    "com_score = completeness_score(merged_df['fried'], kmeans_labels)\n",
    "print (\"Completeness Score :\",com_score.round(3))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "3.3 Evaluation of K-means metrics after Principal Component Analysis"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of components 2\n",
      "Silhouette Score after PCA : 0.369\n",
      "Homogeneity Score after PCA : 0.281\n",
      "V-Measure Score after PCA : 0.266\n",
      "Completeness Score after PCA : 0.253\n",
      "\n",
      "\n",
      "number of components 3\n",
      "Silhouette Score after PCA : 0.282\n",
      "Homogeneity Score after PCA : 0.246\n",
      "V-Measure Score after PCA : 0.231\n",
      "Completeness Score after PCA : 0.218\n",
      "\n",
      "\n",
      "number of components 4\n",
      "Silhouette Score after PCA : 0.229\n",
      "Homogeneity Score after PCA : 0.224\n",
      "V-Measure Score after PCA : 0.211\n",
      "Completeness Score after PCA : 0.2\n",
      "\n",
      "\n",
      "number of components 5\n",
      "Silhouette Score after PCA : 0.201\n",
      "Homogeneity Score after PCA : 0.237\n",
      "V-Measure Score after PCA : 0.223\n",
      "Completeness Score after PCA : 0.211\n",
      "\n",
      "\n",
      "number of components 6\n",
      "Silhouette Score after PCA : 0.181\n",
      "Homogeneity Score after PCA : 0.255\n",
      "V-Measure Score after PCA : 0.24\n",
      "Completeness Score after PCA : 0.227\n",
      "\n",
      "\n",
      "number of components 7\n",
      "Silhouette Score after PCA : 0.161\n",
      "Homogeneity Score after PCA : 0.243\n",
      "V-Measure Score after PCA : 0.229\n",
      "Completeness Score after PCA : 0.217\n",
      "\n",
      "\n",
      "number of components 8\n",
      "Silhouette Score after PCA : 0.143\n",
      "Homogeneity Score after PCA : 0.239\n",
      "V-Measure Score after PCA : 0.225\n",
      "Completeness Score after PCA : 0.212\n",
      "\n",
      "\n",
      "number of components 9\n",
      "Silhouette Score after PCA : 0.133\n",
      "Homogeneity Score after PCA : 0.235\n",
      "V-Measure Score after PCA : 0.222\n",
      "Completeness Score after PCA : 0.21\n",
      "\n",
      "\n",
      "number of components 10\n",
      "Silhouette Score after PCA : 0.123\n",
      "Homogeneity Score after PCA : 0.235\n",
      "V-Measure Score after PCA : 0.222\n",
      "Completeness Score after PCA : 0.21\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(2, 11):\n",
    "    pca = PCA(n_components=i)\n",
    "    pca.fit(merged_df_std)\n",
    "    scores_pca = pca.transform(merged_df_std)\n",
    "\n",
    "    print('number of components', i)\n",
    "\n",
    "    pca_kmeans = KMeans(n_clusters=3)\n",
    "    pca_kmeans.fit(scores_pca)\n",
    "    pca_kmeans_labels = pca_kmeans.predict(scores_pca)\n",
    "\n",
    "    silhouette_pca = silhouette_score(scores_pca, pca_kmeans_labels)\n",
    "    print(\"Silhouette Score after PCA :\", silhouette_pca.round(3))\n",
    "\n",
    "    homogeneity_pca = homogeneity_score(merged_df['fried'], pca_kmeans_labels)\n",
    "    print (\"Homogeneity Score after PCA :\", homogeneity_pca.round(3))\n",
    "\n",
    "    v_measure_pca = v_measure_score(merged_df['fried'], pca_kmeans_labels)\n",
    "    print (\"V-Measure Score after PCA :\", v_measure_pca.round(3))\n",
    "\n",
    "    com_score_pca = completeness_score(merged_df['fried'], pca_kmeans_labels)\n",
    "    print (\"Completeness Score after PCA :\", com_score_pca.round(3))\n",
    "    print('\\n')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}